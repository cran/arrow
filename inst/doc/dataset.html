<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Working with Arrow Datasets and dplyr</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Working with Arrow Datasets and dplyr</h1>



<p>Apache Arrow lets you work efficiently with large, multi-file
datasets. The arrow R package provides a <a href="https://dplyr.tidyverse.org/">dplyr</a> interface to Arrow
Datasets, and other tools for interactive exploration of Arrow data.</p>
<p>This vignette introduces Datasets and shows how to use dplyr to
analyze them.</p>
<div id="example-nyc-taxi-data" class="section level2">
<h2>Example: NYC taxi data</h2>
<p>The <a href="https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page">New
York City taxi trip record data</a> is widely used in big data exercises
and competitions. For demonstration purposes, we have hosted a
Parquet-formatted version of about ten years of the trip data in a
public Amazon S3 bucket.</p>
<p>The total file size is around 37 gigabytes, even in the efficient
Parquet file format. That’s bigger than memory on most people’s
computers, so you can’t just read it all in and stack it into a single
data frame.</p>
<p>In Windows (for R &gt; 3.6) and macOS binary packages, S3 support is
included. On Linux, when installing from source, S3 support is not
enabled by default, and it has additional system requirements. See
<code>vignette(&quot;install&quot;, package = &quot;arrow&quot;)</code> for details. To see
if your arrow installation has S3 support, run:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>arrow<span class="sc">::</span><span class="fu">arrow_with_s3</span>()</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Even with S3 support enabled, network speed will be a bottleneck
unless your machine is located in the same AWS region as the data. So,
for this vignette, we assume that the NYC taxi dataset has been
downloaded locally in an “nyc-taxi” directory.</p>
<div id="retrieving-data-from-a-public-amazon-s3-bucket" class="section level3">
<h3>Retrieving data from a public Amazon S3 bucket</h3>
<p>If your arrow build has S3 support, you can sync the data locally
with:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>arrow<span class="sc">::</span><span class="fu">copy_files</span>(<span class="st">&quot;s3://ursa-labs-taxi-data&quot;</span>, <span class="st">&quot;nyc-taxi&quot;</span>)</span></code></pre></div>
<p>If your arrow build doesn’t have S3 support, you can download the
files with the additional code shown below. Since these are large files,
you may need to increase R’s download timeout from the default of 60
seconds, e.g. <code>options(timeout = 300)</code>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>bucket <span class="ot">&lt;-</span> <span class="st">&quot;https://ursa-labs-taxi-data.s3.us-east-2.amazonaws.com&quot;</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (year <span class="cf">in</span> <span class="dv">2009</span><span class="sc">:</span><span class="dv">2019</span>) {</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (year <span class="sc">==</span> <span class="dv">2019</span>) {</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We only have through June 2019 there</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    months <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    months <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">12</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (month <span class="cf">in</span> <span class="fu">sprintf</span>(<span class="st">&quot;%02d&quot;</span>, months)) {</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">dir.create</span>(<span class="fu">file.path</span>(<span class="st">&quot;nyc-taxi&quot;</span>, year, month), <span class="at">recursive =</span> <span class="cn">TRUE</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">try</span>(<span class="fu">download.file</span>(</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>      <span class="fu">paste</span>(bucket, year, month, <span class="st">&quot;data.parquet&quot;</span>, <span class="at">sep =</span> <span class="st">&quot;/&quot;</span>),</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>      <span class="fu">file.path</span>(<span class="st">&quot;nyc-taxi&quot;</span>, year, month, <span class="st">&quot;data.parquet&quot;</span>),</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>      <span class="at">mode =</span> <span class="st">&quot;wb&quot;</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    ), <span class="at">silent =</span> <span class="cn">TRUE</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Note that these download steps in the vignette are not executed: if
you want to run with live data, you’ll have to do it yourself
separately. Given the size, if you’re running this locally and don’t
have a fast connection, feel free to grab only a year or two of
data.</p>
<p>If you don’t have the taxi data downloaded, the vignette will still
run and will yield previously cached output for reference. To be
explicit about which version is running, let’s check whether you’re
running with live data:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dir.exists</span>(<span class="st">&quot;nyc-taxi&quot;</span>)</span></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
</div>
</div>
<div id="opening-the-dataset" class="section level2">
<h2>Opening the dataset</h2>
<p>Because dplyr is not necessary for many Arrow workflows, it is an
optional (<code>Suggests</code>) dependency. So, to work with Datasets,
you need to load both arrow and dplyr.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(arrow, <span class="at">warn.conflicts =</span> <span class="cn">FALSE</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr, <span class="at">warn.conflicts =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>The first step is to create a Dataset object, pointing at the
directory of data.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>ds <span class="ot">&lt;-</span> <span class="fu">open_dataset</span>(<span class="st">&quot;nyc-taxi&quot;</span>, <span class="at">partitioning =</span> <span class="fu">c</span>(<span class="st">&quot;year&quot;</span>, <span class="st">&quot;month&quot;</span>))</span></code></pre></div>
<p>The file format for <code>open_dataset()</code> is controlled by the
<code>format</code> parameter, which has a default value of
<code>&quot;parquet&quot;</code>. If you had a directory of Arrow format files,
you could instead specify <code>format = &quot;arrow&quot;</code> in the call.</p>
<p>Other supported formats include:</p>
<ul>
<li><code>&quot;feather&quot;</code> or <code>&quot;ipc&quot;</code> (aliases for
<code>&quot;arrow&quot;</code>, as Feather v2 is the Arrow file format)</li>
<li><code>&quot;csv&quot;</code> (comma-delimited files) and <code>&quot;tsv&quot;</code>
(tab-delimited files)</li>
<li><code>&quot;text&quot;</code> (generic text-delimited files - use the
<code>delimiter</code> argument to specify which to use)</li>
</ul>
<p>For text files, you can pass the following parsing options to
<code>open_dataset()</code>:</p>
<ul>
<li><code>delim</code></li>
<li><code>quote</code></li>
<li><code>escape_double</code></li>
<li><code>escape_backslash</code></li>
<li><code>skip_empty_rows</code></li>
</ul>
<p>For more information on the usage of these parameters, see
<code>?read_delim_arrow()</code>.</p>
<p>The <code>partitioning</code> argument lets you specify how the file
paths provide information about how the dataset is chunked into
different files. The files in this example have file paths like</p>
<pre><code>2009/01/data.parquet
2009/02/data.parquet
...</code></pre>
<p>By providing <code>c(&quot;year&quot;, &quot;month&quot;)</code> to the
<code>partitioning</code> argument, you’re saying that the first path
segment gives the value for <code>year</code>, and the second segment is
<code>month</code>. Every row in <code>2009/01/data.parquet</code> has a
value of 2009 for <code>year</code> and 1 for <code>month</code>, even
though those columns may not be present in the file.</p>
<p>Indeed, when you look at the dataset, you can see that in addition to
the columns present in every file, there are also columns
<code>year</code> and <code>month</code> even though they are not
present in the files themselves.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>ds</span></code></pre></div>
<pre><code>## 
## FileSystemDataset with 125 Parquet files
## vendor_id: string
## pickup_at: timestamp[us]
## dropoff_at: timestamp[us]
## passenger_count: int8
## trip_distance: float
## pickup_longitude: float
## pickup_latitude: float
## rate_code_id: null
## store_and_fwd_flag: string
## dropoff_longitude: float
## dropoff_latitude: float
## payment_type: string
## fare_amount: float
## extra: float
## mta_tax: float
## tip_amount: float
## tolls_amount: float
## total_amount: float
## year: int32
## month: int32
## 
## See $metadata for additional Schema metadata</code></pre>
<p>The other form of partitioning currently supported is <a href="https://hive.apache.org/">Hive</a>-style, in which the partition
variable names are included in the path segments. If you had saved your
files in paths like:</p>
<pre><code>year=2009/month=01/data.parquet
year=2009/month=02/data.parquet
...</code></pre>
<p>you would not have had to provide the names in
<code>partitioning</code>; you could have just called
<code>ds &lt;- open_dataset(&quot;nyc-taxi&quot;)</code> and the partitions would
have been detected automatically.</p>
</div>
<div id="querying-the-dataset" class="section level2">
<h2>Querying the dataset</h2>
<p>Up to this point, you haven’t loaded any data. You’ve walked
directories to find files, you’ve parsed file paths to identify
partitions, and you’ve read the headers of the Parquet files to inspect
their schemas so that you can make sure they all are as expected.</p>
<p>In the current release, arrow supports the dplyr verbs
<code>mutate()</code>, <code>transmute()</code>, <code>select()</code>,
<code>rename()</code>, <code>relocate()</code>, <code>filter()</code>,
and <code>arrange()</code>. Aggregation is not yet supported, so before
you call <code>summarise()</code> or other verbs with aggregate
functions, use <code>collect()</code> to pull the selected subset of the
data into an in-memory R data frame.</p>
<p>Suppose you attempt to call unsupported dplyr verbs or unimplemented
functions in your query on an Arrow Dataset. In that case, the arrow
package raises an error. However, for dplyr queries on Arrow Table
objects (which are already in memory), the package automatically calls
<code>collect()</code> before processing that dplyr verb.</p>
<p>Here’s an example: suppose that you are curious about tipping
behavior among the longest taxi rides. Let’s find the median tip
percentage for rides with fares greater than $100 in 2015, broken down
by the number of passengers:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">system.time</span>(ds <span class="sc">%&gt;%</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(total_amount <span class="sc">&gt;</span> <span class="dv">100</span>, year <span class="sc">==</span> <span class="dv">2015</span>) <span class="sc">%&gt;%</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(tip_amount, total_amount, passenger_count) <span class="sc">%&gt;%</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tip_pct =</span> <span class="dv">100</span> <span class="sc">*</span> tip_amount <span class="sc">/</span> total_amount) <span class="sc">%&gt;%</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(passenger_count) <span class="sc">%&gt;%</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">collect</span>() <span class="sc">%&gt;%</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">median_tip_pct =</span> <span class="fu">median</span>(tip_pct),</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">n =</span> <span class="fu">n</span>()</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>())</span></code></pre></div>
<pre><code>## 
## # A tibble: 10 x 3
##    passenger_count median_tip_pct      n
##              &lt;int&gt;          &lt;dbl&gt;  &lt;int&gt;
##  1               0           9.84    380
##  2               1          16.7  143087
##  3               2          16.6   34418
##  4               3          14.4    8922
##  5               4          11.4    4771
##  6               5          16.7    5806
##  7               6          16.7    3338
##  8               7          16.7      11
##  9               8          16.7      32
## 10               9          16.7      42
## 
##    user  system elapsed
##   4.436   1.012   1.402</code></pre>
<p>You’ve just selected a subset out of a dataset with around 2 billion
rows, computed a new column, and aggregated it in under 2 seconds on a
modern laptop. How does this work?</p>
<p>First, <code>mutate()</code>/<code>transmute()</code>,
<code>select()</code>/<code>rename()</code>/<code>relocate()</code>,
<code>filter()</code>, <code>group_by()</code>, and
<code>arrange()</code> record their actions but don’t evaluate on the
data until you run <code>collect()</code>.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>ds <span class="sc">%&gt;%</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(total_amount <span class="sc">&gt;</span> <span class="dv">100</span>, year <span class="sc">==</span> <span class="dv">2015</span>) <span class="sc">%&gt;%</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(tip_amount, total_amount, passenger_count) <span class="sc">%&gt;%</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tip_pct =</span> <span class="dv">100</span> <span class="sc">*</span> tip_amount <span class="sc">/</span> total_amount) <span class="sc">%&gt;%</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(passenger_count)</span></code></pre></div>
<pre><code>## 
## FileSystemDataset (query)
## tip_amount: float
## total_amount: float
## passenger_count: int8
## tip_pct: expr
## 
## * Filter: ((total_amount &gt; 100) and (year == 2015))
## * Grouped by passenger_count
## See $.data for the source Arrow object</code></pre>
<p>This code returns an output instantly and shows the manipulations
you’ve made, without loading data from the files. Because the evaluation
of these queries is deferred, you can build up a query that selects down
to a small subset without generating intermediate datasets that would
potentially be large.</p>
<p>Second, all work is pushed down to the individual data files, and
depending on the file format, chunks of data within the files. As a
result, you can select a subset of data from a much larger dataset by
collecting the smaller slices from each file—you don’t have to load the
whole dataset in memory to slice from it.</p>
<p>Third, because of partitioning, you can ignore some files entirely.
In this example, by filtering <code>year == 2015</code>, all files
corresponding to other years are immediately excluded: you don’t have to
load them in order to find that no rows match the filter. Relatedly,
since Parquet files contain row groups with statistics on the data
within, there may be entire chunks of data you can avoid scanning
because they have no rows where <code>total_amount &gt; 100</code>.</p>
<div id="processing-data-in-batches" class="section level3">
<h3>Processing data in batches</h3>
<p>Sometimes you want to run R code on the entire dataset, but that
dataset is much larger than memory. You can use <code>map_batches</code>
on a dataset query to process it batch-by-batch.</p>
<p><strong>Note</strong>: <code>map_batches</code> is experimental and
not recommended for production use.</p>
<p>As an example, to randomly sample a dataset, use
<code>map_batches</code> to sample a percentage of rows from each
batch:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>sampled_data <span class="ot">&lt;-</span> ds <span class="sc">%&gt;%</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(year <span class="sc">==</span> <span class="dv">2015</span>) <span class="sc">%&gt;%</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(tip_amount, total_amount, passenger_count) <span class="sc">%&gt;%</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map_batches</span>(<span class="sc">~</span> <span class="fu">sample_frac</span>(<span class="fu">as.data.frame</span>(.), <span class="fl">1e-4</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tip_pct =</span> tip_amount <span class="sc">/</span> total_amount)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(sampled_data)</span></code></pre></div>
<pre><code>## 
## &#39;data.frame&#39;:    15603 obs. of  4 variables:
##  $ tip_amount     : num  0 0 1.55 1.45 5.2 ...
##  $ total_amount   : num  5.8 16.3 7.85 8.75 26 ...
##  $ passenger_count: int  1 1 1 1 1 6 5 1 2 1 ...
##  $ tip_pct        : num  0 0 0.197 0.166 0.2 ...</code></pre>
<p>This function can also be used to aggregate summary statistics over a
dataset by computing partial results for each batch and then aggregating
those partial results. Extending the example above, you could fit a
model to the sample data and then use <code>map_batches</code> to
compute the MSE on the full dataset.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(tip_pct <span class="sc">~</span> total_amount <span class="sc">+</span> passenger_count, <span class="at">data =</span> sampled_data)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>ds <span class="sc">%&gt;%</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(year <span class="sc">==</span> <span class="dv">2015</span>) <span class="sc">%&gt;%</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(tip_amount, total_amount, passenger_count) <span class="sc">%&gt;%</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tip_pct =</span> tip_amount <span class="sc">/</span> total_amount) <span class="sc">%&gt;%</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map_batches</span>(<span class="cf">function</span>(batch) {</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    batch <span class="sc">%&gt;%</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>      <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">pred_tip_pct =</span> <span class="fu">predict</span>(model, <span class="at">newdata =</span> .)) <span class="sc">%&gt;%</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.nan</span>(tip_pct)) <span class="sc">%&gt;%</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>      <span class="fu">summarize</span>(<span class="at">sse_partial =</span> <span class="fu">sum</span>((pred_tip_pct <span class="sc">-</span> tip_pct)<span class="sc">^</span><span class="dv">2</span>), <span class="at">n_partial =</span> <span class="fu">n</span>())</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>  }) <span class="sc">%&gt;%</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">mse =</span> <span class="fu">sum</span>(sse_partial) <span class="sc">/</span> <span class="fu">sum</span>(n_partial)) <span class="sc">%&gt;%</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>(mse)</span></code></pre></div>
<pre><code>## 
## [1] 0.1304284</code></pre>
</div>
</div>
<div id="more-dataset-options" class="section level2">
<h2>More dataset options</h2>
<p>There are a few ways you can control the Dataset creation to adapt to
special use cases.</p>
<div id="work-with-files-in-a-directory" class="section level3">
<h3>Work with files in a directory</h3>
<p>If you are working with a single file or a set of files that are not
all in the same directory, you can provide a file path or a vector of
multiple file paths to <code>open_dataset()</code>. This is useful if,
for example, you have a single CSV file that is too big to read into
memory. You could pass the file path to <code>open_dataset()</code>, use
<code>group_by()</code> to partition the Dataset into manageable chunks,
then use <code>write_dataset()</code> to write each chunk to a separate
Parquet file—all without needing to read the full CSV file into R.</p>
</div>
<div id="explicitly-declare-column-names-and-data-types" class="section level3">
<h3>Explicitly declare column names and data types</h3>
<p>You can specify the <code>schema</code> argument to
<code>open_dataset()</code> to declare the columns and their data types.
This is useful if you have data files that have different storage schema
(for example, a column could be <code>int32</code> in one and
<code>int8</code> in another) and you want to ensure that the resulting
Dataset has a specific type.</p>
<p>To be clear, it’s not necessary to specify a schema, even in this
example of mixed integer types, because the Dataset constructor will
reconcile differences like these. The schema specification just lets you
declare what you want the result to be.</p>
</div>
<div id="explicitly-declare-partition-format" class="section level3">
<h3>Explicitly declare partition format</h3>
<p>Similarly, you can provide a Schema in the <code>partitioning</code>
argument of <code>open_dataset()</code> in order to declare the types of
the virtual columns that define the partitions. This would be useful, in
the taxi dataset example, if you wanted to keep <code>month</code> as a
string instead of an integer.</p>
</div>
<div id="work-with-multiple-data-sources" class="section level3">
<h3>Work with multiple data sources</h3>
<p>Another feature of Datasets is that they can be composed of multiple
data sources. That is, you may have a directory of partitioned Parquet
files in one location, and in another directory, files that haven’t been
partitioned. Or, you could point to an S3 bucket of Parquet data and a
directory of CSVs on the local file system and query them together as a
single dataset. To create a multi-source dataset, provide a list of
datasets to <code>open_dataset()</code> instead of a file path, or
simply concatenate them like
<code>big_dataset &lt;- c(ds1, ds2)</code>.</p>
</div>
</div>
<div id="writing-datasets" class="section level2">
<h2>Writing datasets</h2>
<p>As you can see, querying a large dataset can be made quite fast by
storage in an efficient binary columnar format like Parquet or Feather
and partitioning based on columns commonly used for filtering. However,
data isn’t always stored that way. Sometimes you might start with one
giant CSV. The first step in analyzing data is cleaning is up and
reshaping it into a more usable form.</p>
<p>The <code>write_dataset()</code> function allows you to take a
Dataset or another tabular data object—an Arrow Table or RecordBatch, or
an R data frame—and write it to a different file format, partitioned
into multiple files.</p>
<p>Assume that you have a version of the NYC Taxi data as CSV:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>ds <span class="ot">&lt;-</span> <span class="fu">open_dataset</span>(<span class="st">&quot;nyc-taxi/csv/&quot;</span>, <span class="at">format =</span> <span class="st">&quot;csv&quot;</span>)</span></code></pre></div>
<p>You can write it to a new location and translate the files to the
Feather format by calling <code>write_dataset()</code> on it:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">write_dataset</span>(ds, <span class="st">&quot;nyc-taxi/feather&quot;</span>, <span class="at">format =</span> <span class="st">&quot;feather&quot;</span>)</span></code></pre></div>
<p>Next, let’s imagine that the <code>payment_type</code> column is
something you often filter on, so you want to partition the data by that
variable. By doing so you ensure that a filter like
<code>payment_type == &quot;Cash&quot;</code> will touch only a subset of files
where <code>payment_type</code> is always <code>&quot;Cash&quot;</code>.</p>
<p>One natural way to express the columns you want to partition on is to
use the <code>group_by()</code> method:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>ds <span class="sc">%&gt;%</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(payment_type) <span class="sc">%&gt;%</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">write_dataset</span>(<span class="st">&quot;nyc-taxi/feather&quot;</span>, <span class="at">format =</span> <span class="st">&quot;feather&quot;</span>)</span></code></pre></div>
<p>This will write files to a directory tree that looks like this:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">system</span>(<span class="st">&quot;tree nyc-taxi/feather&quot;</span>)</span></code></pre></div>
<pre><code>## feather
## ├── payment_type=1
## │   └── part-18.feather
## ├── payment_type=2
## │   └── part-19.feather
## ...
## └── payment_type=UNK
##     └── part-17.feather
##
## 18 directories, 23 files</code></pre>
<p>Note that the directory names are <code>payment_type=Cash</code> and
similar: this is the Hive-style partitioning described above. This means
that when you call <code>open_dataset()</code> on this directory, you
don’t have to declare what the partitions are because they can be read
from the file paths. (To instead write bare values for partition
segments, i.e. <code>Cash</code> rather than
<code>payment_type=Cash</code>, call <code>write_dataset()</code> with
<code>hive_style = FALSE</code>.)</p>
<p>Perhaps, though, <code>payment_type == &quot;Cash&quot;</code> is the only data
you ever care about, and you just want to drop the rest and have a
smaller working set. For this, you can <code>filter()</code> them out
when writing:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>ds <span class="sc">%&gt;%</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(payment_type <span class="sc">==</span> <span class="st">&quot;Cash&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">write_dataset</span>(<span class="st">&quot;nyc-taxi/feather&quot;</span>, <span class="at">format =</span> <span class="st">&quot;feather&quot;</span>)</span></code></pre></div>
<p>The other thing you can do when writing datasets is select a subset
of columns or reorder them. Suppose you never care about
<code>vendor_id</code>, and being a string column, it can take up a lot
of space when you read it in, so let’s drop it:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>ds <span class="sc">%&gt;%</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(payment_type) <span class="sc">%&gt;%</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>vendor_id) <span class="sc">%&gt;%</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">write_dataset</span>(<span class="st">&quot;nyc-taxi/feather&quot;</span>, <span class="at">format =</span> <span class="st">&quot;feather&quot;</span>)</span></code></pre></div>
<p>Note that while you can select a subset of columns, you cannot
currently rename columns when writing a dataset.</p>
</div>
<div id="partitioning-performance-considerations" class="section level2">
<h2>Partitioning performance considerations</h2>
<p>Partitioning datasets has two aspects that affect performance: it
increases the number of files and it creates a directory structure
around the files. Both of these have benefits as well as costs.
Depending on the configuration and the size of your dataset, the costs
can outweigh the benefits.</p>
<p>Because partitions split up the dataset into multiple files,
partitioned datasets can be read and written with parallelism. However,
each additional file adds a little overhead in processing for filesystem
interaction. It also increases the overall dataset size since each file
has some shared metadata. For example, each parquet file contains the
schema and group-level statistics. The number of partitions is a floor
for the number of files. If you partition a dataset by date with a year
of data, you will have at least 365 files. If you further partition by
another dimension with 1,000 unique values, you will have up to 365,000
files. This fine of partitioning often leads to small files that mostly
consist of metadata.</p>
<p>Partitioned datasets create nested folder structures, and those allow
us to prune which files are loaded in a scan. However, this adds
overhead to discovering files in the dataset, as we’ll need to
recursively “list directory” to find the data files. Too fine partitions
can cause problems here: Partitioning a dataset by date for a years
worth of data will require 365 list calls to find all the files; adding
another column with cardinality 1,000 will make that 365,365 calls.</p>
<p>The most optimal partitioning layout will depend on your data, access
patterns, and which systems will be reading the data. Most systems,
including Arrow, should work across a range of file sizes and
partitioning layouts, but there are extremes you should avoid. These
guidelines can help avoid some known worst cases:</p>
<ul>
<li>Avoid files smaller than 20MB and larger than 2GB.</li>
<li>Avoid partitioning layouts with more than 10,000 distinct
partitions.</li>
</ul>
<p>For file formats that have a notion of groups within a file, such as
Parquet, similar guidelines apply. Row groups can provide parallelism
when reading and allow data skipping based on statistics, but very small
groups can cause metadata to be a significant portion of file size.
Arrow’s file writer provides sensible defaults for group sizing in most
cases.</p>
</div>
<div id="transactions-acid-guarantees" class="section level2">
<h2>Transactions / ACID guarantees</h2>
<p>The dataset API offers no transaction support or any ACID guarantees.
This affects both reading and writing. Concurrent reads are fine.
Concurrent writes or writes concurring with reads may have unexpected
behavior. Various approaches can be used to avoid operating on the same
files such as using a unique basename template for each writer, a
temporary directory for new files, or separate storage of the file list
instead of relying on directory discovery.</p>
<p>Unexpectedly killing the process while a write is in progress can
leave the system in an inconsistent state. Write calls generally return
as soon as the bytes to be written have been completely delivered to the
OS page cache. Even though a write operation has been completed it is
possible for part of the file to be lost if there is a sudden power loss
immediately after the write call.</p>
<p>Most file formats have magic numbers which are written at the end.
This means a partial file write can safely be detected and discarded.
The CSV file format does not have any such concept and a partially
written CSV file may be detected as valid.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
